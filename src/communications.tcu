/*---------------------------------------------------------------------------*\
 *
 *  minimmerflow
 *
 *  Copyright (C) 2015-2022 OPTIMAD engineering Srl
 *
 *  -------------------------------------------------------------------------
 *  License
 *  This file is part of minimmerflow.
 *
 *  minimmerflow is free software: you can redistribute it and/or modify it
 *  under the terms of the GNU Lesser General Public License v3 (LGPL)
 *  as published by the Free Software Foundation.
 *
 *  minimmerflow is distributed in the hope that it will be useful, but WITHOUT
 *  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 *  FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public
 *  License for more details.
 *
 *  You should have received a copy of the GNU Lesser General Public License
 *  along with minimmerflow. If not, see <http://www.gnu.org/licenses/>.
 *
\*---------------------------------------------------------------------------*/

#ifndef __MINIMMERFLOW_COMMUNICATIONS_TCU__
#define __MINIMMERFLOW_COMMUNICATIONS_TCU__

#include <nvtx3/nvToolsExt.h>

/*!
    Write the dataset to the buffer.

    \param rank is the rank of the process who will receive the data
    \param buffer is the buffer where the data will be written to
    \param list is the list of ids that will be written
*/
template<typename container_t>
void CudaStorageBufferStreamer<container_t>::write(const int &rank, bitpit::SendBuffer &CPUbuffer,
                                            const std::vector<long> &list)
{
    container_t &container = this->getContainer();
    auto & rankContainer = container[rank];
    std::size_t rankContainerSize = rankContainer.size();
    //std::vector<double> temp(rankContainerSize, 10.0);
    nvtxRangePushA("STREAMER_WRITE_CUDAMCPY");
    cudaMemcpy(m_temp.data(), rankContainer.cuda_deviceData(), rankContainer.cuda_deviceDataSize() * sizeof(typename container_t::mapped_type::dev_value_type), cudaMemcpyDeviceToHost);
    nvtxRangePop();
    nvtxRangePushA("STREAMER_WRITE_BUFFERSTREAMING");
    for (size_t i = 0; i < rankContainerSize; ++i) {
        CPUbuffer << m_temp[i];
    }
//    for (double t : temp) {
//        CPUbuffer << t;
//    }
    nvtxRangePop();

}

template<typename container_t>
CudaStorageBufferStreamer<container_t>::CudaStorageBufferStreamer(container_t *container, const size_t & readOffset, const size_t & writeOffset, const size_t &itemSize, size_t writeBufferSize)
    : BaseListBufferStreamer<container_t>(container, itemSize),
      m_readOffset(readOffset), m_writeOffset(writeOffset), m_maxBufferSize(writeBufferSize), m_temp(writeBufferSize, 0.0)
{
    size_t bytes = writeBufferSize * sizeof(double);
    cudaError_t err = cudaHostRegister(m_temp.data(), bytes, cudaHostRegisterDefault);
    if (err != cudaSuccess) {
        std::cerr << "CUDA runtime error in cudaHostRegister " << cudaGetErrorString(err) << std::endl;
    }

}

template<typename container_t>
CudaStorageBufferStreamer<container_t>::~CudaStorageBufferStreamer()
{
    cudaError_t err = cudaHostUnregister(m_temp.data());
    if (err != cudaSuccess) {
        std::cerr << "CUDA runtime error in cudaHostUnregister " << cudaGetErrorString(err) << std::endl;
    }
}

#endif
