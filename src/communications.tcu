/*---------------------------------------------------------------------------*\
 *
 *  minimmerflow
 *
 *  Copyright (C) 2015-2022 OPTIMAD engineering Srl
 *
 *  -------------------------------------------------------------------------
 *  License
 *  This file is part of minimmerflow.
 *
 *  minimmerflow is free software: you can redistribute it and/or modify it
 *  under the terms of the GNU Lesser General Public License v3 (LGPL)
 *  as published by the Free Software Foundation.
 *
 *  minimmerflow is distributed in the hope that it will be useful, but WITHOUT
 *  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 *  FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public
 *  License for more details.
 *
 *  You should have received a copy of the GNU Lesser General Public License
 *  along with minimmerflow. If not, see <http://www.gnu.org/licenses/>.
 *
\*---------------------------------------------------------------------------*/

#ifndef __MINIMMERFLOW_COMMUNICATIONS_TCU__
#define __MINIMMERFLOW_COMMUNICATIONS_TCU__

#include <nvtx3/nvToolsExt.h>
#include <bitpit_communications.hpp>

#include <iostream>
#include <fstream>

/*!
    Write the dataset to the buffer.

    \param rank is the rank of the process who will receive the data
    \param buffer is the buffer where the data will be written to
    \param list is the list of ids that will be written
*/
template<typename container_t>
void CudaStorageBufferStreamer<container_t>::write(const int &rank, bitpit::SendBuffer &CPUbuffer,
                                            const std::vector<long> &list)
{
    container_t &container = this->getContainer();
    auto & rankContainer = container[rank];
    std::size_t rankContainerSize = rankContainer.size();

    nvtxRangePushA("STREAMER_WRITE_CUDAMCPY");
    cudaMemcpyAsync(CPUbuffer.getFront().data() + (m_rankOffsets.at(rank) * sizeof(typename container_t::mapped_type::dev_value_type)),
            rankContainer.cuda_deviceData(), rankContainer.cuda_deviceDataSize() * sizeof(typename container_t::mapped_type::dev_value_type),
            cudaMemcpyDeviceToHost, m_cudaStreams[rank]);
//    std::cout << " Offset = " << m_rankOffsets.at(rank) << std::endl;
    nvtxRangePop();
}

template<typename container_t>
void CudaStorageBufferStreamer<container_t>::finalizeWrite(const int &rank, bitpit::SendBuffer &CPUbuffer, const std::vector<long> &list)
{
    //Do nothing
    BITPIT_UNUSED(rank);
    BITPIT_UNUSED(CPUbuffer);
    BITPIT_UNUSED(list);
}

/*!
    Read the dataset from the buffer.

    \param rank is the rank of the process who sent the data
    \param buffer is the buffer where the data will be read from
    \param list is the list of ids that will be read
*/
template<typename container_t>
void CudaStorageBufferStreamer<container_t>::read(int const &rank, bitpit::RecvBuffer &CPUbuffer,
                                           const std::vector<long> &list)
{
    //Do nothing
    BITPIT_UNUSED(rank);
    BITPIT_UNUSED(CPUbuffer);
    BITPIT_UNUSED(list);
}



template<typename container_t>
CudaStorageBufferStreamer<container_t>::CudaStorageBufferStreamer(container_t *container, const size_t & readOffset, const size_t & writeOffset, const size_t &itemSize,
        const std::unordered_map<int, size_t> & rankOffsets, int fieldId)
    : BaseListBufferStreamer<container_t>(container, itemSize),
      m_readOffset(readOffset), m_writeOffset(writeOffset), m_cudaStreams(0)
{
    for (auto offset : rankOffsets) {
        m_rankOffsets[offset.first] = offset.second * fieldId;
    }
}

template<typename container_t>
CudaStorageBufferStreamer<container_t>::~CudaStorageBufferStreamer()
{
    for (auto & rankContainer : *(this->m_container)) {
        int rank = rankContainer.first;
        cudaStreamDestroy(m_cudaStreams[rank]);
    }
}

template<typename container_t>
void CudaStorageBufferStreamer<container_t>::initializeCUDAObjects()
{
    for (auto & rankContainer : *(this->m_container)) {
        int rank = rankContainer.first;
        if (m_cudaStreams.find(rank) == m_cudaStreams.end()) {
            m_cudaStreams[rank] = cudaStream_t();
            cudaStreamCreate(&m_cudaStreams[rank]);
        }

    }

    m_cudaStreams.reserve((*(this->m_container)).size());
}


/*!
    Write the dataset to the buffer.

    \param rank is the rank of the process who will receive the data
    \param buffer is the buffer where the data will be written to
    \param list is the list of ids that will be written
*/
template<typename container_t>
void CudaStorageCollectionBufferStreamer<container_t>::write(const int &rank, bitpit::SendBuffer &CPUbuffer,
                                            const std::vector<long> &list)
{
    container_t &container = this->getContainer();
    auto & rankContainer = container[rank];
    std::size_t rankContainerSize = rankContainer.size();

    nvtxRangePushA("STREAMER_WRITE_CUDAMCPY");
    cudaMemcpyAsync(CPUbuffer.getFront().data(), rankContainer.cuda_deviceData(), rankContainer.cuda_deviceDataSize() * sizeof(typename container_t::mapped_type::dev_value_type),
            cudaMemcpyDeviceToHost, m_cudaStreams[rank]);
    nvtxRangePop();
}

template<typename container_t>
void CudaStorageCollectionBufferStreamer<container_t>::finalizeWrite(const int &rank, bitpit::SendBuffer &CPUbuffer, const std::vector<long> &list)
{
    //Do nothing
    BITPIT_UNUSED(rank);
    BITPIT_UNUSED(CPUbuffer);
    BITPIT_UNUSED(list);
}

/*!
    Read the dataset from the buffer.

    \param rank is the rank of the process who sent the data
    \param buffer is the buffer where the data will be read from
    \param list is the list of ids that will be read
*/
template<typename container_t>
void CudaStorageCollectionBufferStreamer<container_t>::read(int const &rank, bitpit::RecvBuffer &CPUbuffer,
                                           const std::vector<long> &list)
{
    //Do nothing
//    BITPIT_UNUSED(rank);
//    BITPIT_UNUSED(CPUbuffer);
//    BITPIT_UNUSED(list);

    container_t &container = this->getContainer();
    auto & rankContainer = container[rank];
    std::size_t rankContainerSize = rankContainer.size();

    nvtxRangePushA("STREAMER_READ_CUDAMCPY");
    cudaMemcpyAsync(rankContainer.cuda_deviceData(), CPUbuffer.getFront().data(), rankContainer.cuda_deviceDataSize() * sizeof(typename container_t::mapped_type::dev_value_type),
            cudaMemcpyHostToDevice, m_cudaStreams[rank]);
    nvtxRangePop();


}


template<typename container_t>
CudaStorageCollectionBufferStreamer<container_t>::CudaStorageCollectionBufferStreamer(container_t *container, const size_t & readOffset, const size_t & writeOffset, const size_t &itemSize)
    : BaseListBufferStreamer<container_t>(container, itemSize),
      m_readOffset(readOffset), m_writeOffset(writeOffset), m_cudaStreams(0), m_deviceStorage(nullptr), m_targetLists(nullptr)
{
}

template<typename container_t>
CudaStorageCollectionBufferStreamer<container_t>::~CudaStorageCollectionBufferStreamer()
{
//    for (auto & rankContainer : *(this->m_container)) {
//        int rank = rankContainer.first;
//        cudaStreamDestroy(m_cudaStreams[rank]);
//    }
    for (auto & stream : m_cudaStreams) {
        cudaStreamDestroy(stream.second);
    }
}

template<typename container_t>
void CudaStorageCollectionBufferStreamer<container_t>::initializeCUDAObjects()
{
    for (auto & rankContainer : *(this->m_container)) {
        int rank = rankContainer.first;
        if (m_cudaStreams.find(rank) == m_cudaStreams.end()) {
            m_cudaStreams[rank] = cudaStream_t();
            cudaStreamCreate(&m_cudaStreams[rank]);
        }

    }

    m_cudaStreams.reserve((*(this->m_container)).size());
}

template<typename container_t>
void CudaStorageCollectionBufferStreamer<container_t>::initializePointers(ScalarPiercedStorageCollection<double> * storage,
        std::unordered_map<int, ScalarStorage<std::size_t>> * targets)
{
    m_deviceStorage = storage;
    m_targetLists = targets;
}
#endif
