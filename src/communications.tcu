/*---------------------------------------------------------------------------*\
 *
 *  minimmerflow
 *
 *  Copyright (C) 2015-2022 OPTIMAD engineering Srl
 *
 *  -------------------------------------------------------------------------
 *  License
 *  This file is part of minimmerflow.
 *
 *  minimmerflow is free software: you can redistribute it and/or modify it
 *  under the terms of the GNU Lesser General Public License v3 (LGPL)
 *  as published by the Free Software Foundation.
 *
 *  minimmerflow is distributed in the hope that it will be useful, but WITHOUT
 *  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 *  FITNESS FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public
 *  License for more details.
 *
 *  You should have received a copy of the GNU Lesser General Public License
 *  along with minimmerflow. If not, see <http://www.gnu.org/licenses/>.
 *
\*---------------------------------------------------------------------------*/

#ifndef __MINIMMERFLOW_COMMUNICATIONS_TCU__
#define __MINIMMERFLOW_COMMUNICATIONS_TCU__

#include <nvtx3/nvToolsExt.h>
#include <bitpit_communications.hpp>

/*!
    Write the dataset to the buffer.

    \param rank is the rank of the process who will receive the data
    \param buffer is the buffer where the data will be written to
    \param list is the list of ids that will be written
*/
template<typename container_t>
void CudaStorageBufferStreamer<container_t>::write(const int &rank, bitpit::SendBuffer &CPUbuffer,
                                            const std::vector<long> &list)
{
    container_t &container = this->getContainer();
    auto & rankContainer = container[rank];
    std::size_t rankContainerSize = rankContainer.size();
    //std::vector<double> temp(rankContainerSize, 10.0);

    nvtxRangePushA("STREAMER_WRITE_CUDAMCPY");
    cudaMemcpyAsync(m_temp[rank].data(), rankContainer.cuda_deviceData(), rankContainer.cuda_deviceDataSize() * sizeof(typename container_t::mapped_type::dev_value_type),
            cudaMemcpyDeviceToHost, m_cudaStreams[rank]);
//    cudaMemcpyAsync(m_temp[rank].data(), rankContainer.cuda_deviceData(), rankContainer.cuda_deviceDataSize() * sizeof(typename container_t::mapped_type::dev_value_type),
//            cudaMemcpyDeviceToHost, m_cudaStreams[rank]);
//    cudaMemcpy(m_temp[rank].data(), rankContainer.cuda_deviceData(), rankContainer.cuda_deviceDataSize() * sizeof(typename container_t::mapped_type::dev_value_type),
//            cudaMemcpyDeviceToHost);
    nvtxRangePop();
}

template<typename container_t>
void CudaStorageBufferStreamer<container_t>::finalizeWrite(const int &rank, bitpit::SendBuffer &CPUbuffer, const std::vector<long> &list)
{
    //Do nothing
    //cudaStreamSynchronize(m_cudaStreams[rank]);
    nvtxRangePushA("STREAMER_WRITE_BUFFERSTREAMING");
    container_t &container = this->getContainer();
    auto & rankContainer = container[rank];
    std::size_t rankContainerSize = rankContainer.size();
    for (size_t i = 0; i < rankContainerSize; ++i) {
        CPUbuffer << m_temp[rank][i];
    }
    nvtxRangePop();
}

/*!
    Read the dataset from the buffer.

    \param rank is the rank of the process who sent the data
    \param buffer is the buffer where the data will be read from
    \param list is the list of ids that will be read
*/
template<typename container_t>
void CudaStorageBufferStreamer<container_t>::read(int const &rank, bitpit::RecvBuffer &CPUbuffer,
                                           const std::vector<long> &list)
{
    BITPIT_UNUSED(rank);

//    container_t &container = this->getContainer();
//    for (const long k : list) {
//        buffer >> container[k];
//    }
}



template<typename container_t>
CudaStorageBufferStreamer<container_t>::CudaStorageBufferStreamer(container_t *container, const size_t & readOffset, const size_t & writeOffset, const size_t &itemSize, size_t writeBufferSize)
    : BaseListBufferStreamer<container_t>(container, itemSize),
      m_readOffset(readOffset), m_writeOffset(writeOffset), m_maxBufferSize(writeBufferSize), m_cudaStreams(0), m_currentCudaStream(-1)
{
}

template<typename container_t>
CudaStorageBufferStreamer<container_t>::~CudaStorageBufferStreamer()
{
    for (auto & rankContainer : *(this->m_container)) {
        int rank = rankContainer.first;
        cudaStreamDestroy(m_cudaStreams[rank]);
        cudaError_t err = cudaHostUnregister(m_temp[rank].data());
        if (err != cudaSuccess) {
            std::cerr << "CUDA runtime error in cudaHostUnregister " << cudaGetErrorString(err) << std::endl;
        }
    }
}

template<typename container_t>
void CudaStorageBufferStreamer<container_t>::initializeCUDAObjects()
{
    for (auto & rankContainer : *(this->m_container)) {
        int rank = rankContainer.first;
        m_temp[rank] = std::vector<double>(m_maxBufferSize, 0.0);

        size_t bytes = m_maxBufferSize * sizeof(double);
        cudaError_t err = cudaHostRegister(m_temp[rank].data(), bytes, cudaHostRegisterDefault);
        if (err != cudaSuccess) {
            std::cerr << "CUDA runtime error in cudaHostRegister " << cudaGetErrorString(err) << " on buffer for rank " << rank << std::endl;
        }

        if (m_cudaStreams.find(rank) == m_cudaStreams.end()) {
            m_cudaStreams[rank] = cudaStream_t();
            cudaStreamCreate(&m_cudaStreams[rank]);
        }

    }

    m_cudaStreams.reserve((*(this->m_container)).size());
}

/*!
 * Send ghosts data using non-blocking communications
 *
 * \param cellData is the container of the cell data
 */
void ListCommunicator::startAllExchanges()
{
    if (getCommunicator() == MPI_COMM_NULL || !hasData()) {
        return;
    }

    // Start the receives
    for (int rank : getRecvRanks()) {
        if (!isRecvActive(rank)) {
            startRecv(rank);
        }
    }

    // Wait previous sends
    waitAllSends();

    // Fill the buffer with the given field and start sending the data
    for (int rank : getSendRanks()) {
        // Get send buffer
        bitpit::SendBuffer &buffer = getSendBuffer(rank);

        // Write the buffer
        for (ExchangeBufferStreamer *streamer : m_writers) {
            streamer->write(rank, buffer, getStreamableSendList(rank, streamer));
        }

        for (ExchangeBufferStreamer *streamer : m_writers) {
            cudaStreamSynchronize(static_cast<CudaStorageBufferStreamer<std::unordered_map<int, ScalarStorage<double>>>*>(streamer)->m_cudaStreams[rank]);
        }

        for (ExchangeBufferStreamer *streamer : m_writers) {
            streamer->finalizeWrite(rank, buffer, getStreamableSendList(rank, streamer));
        }

        cudaDeviceSynchronize();

        // Start the send
        startSend(rank);
    }
}



#endif
